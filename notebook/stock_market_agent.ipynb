{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewwgordon/autogen-agent-demo/blob/main/notebook/stock_market_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4J2nIYhfYA1"
      },
      "source": [
        "# AutoGen Simple Bi-Directional Conversational Agent Architecture Demonstration\n",
        "\n",
        "In this notebook, we demonstrate how to use `AssistantAgent` and `UserProxyAgent` to write code and execute the code. Here `AssistantAgent` is an LLM-based agent that can write Python code (in a Python coding block) for a user to execute for a given task. `UserProxyAgent` is an agent which serves as a proxy for the human user to execute the code written by `AssistantAgent`, or automatically execute the code. Depending on the setting of `human_input_mode` and `max_consecutive_auto_reply`, the `UserProxyAgent` either solicits feedback from the human user or returns auto-feedback based on the result of code execution (success or failure and corresponding outputs) to `AssistantAgent`. `AssistantAgent` will debug the code and suggest new code if the result contains error. The two agents keep communicating to each other until the task is done.\n",
        "\n",
        "````{=mdx}\n",
        ":::info Requirements\n",
        "Install the following packages before running the code below:\n",
        "```bash\n",
        "pip install pyautogen matplotlib yfinance\n",
        "```\n",
        "\n",
        "For more information, please refer to the [installation guide](/docs/installation/).\n",
        ":::\n",
        "````"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As well as the core Microsoft AutoGen library, we also need a couple of Python libraries to support the Agents\n",
        "# In this case Yahoo Finance to get stock market information and matplotlib for charting.\n",
        "#\n",
        "!pip install pyautogen matplotlib yfinance"
      ],
      "metadata": {
        "id": "mPAPzBnCgCed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get an OpenAI API key\n",
        "import getpass\n",
        "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API Key:\")\n",
        "config_list = [{\"model\": \"gpt-4o-mini\", \"api_key\": OPENAI_API_KEY}]"
      ],
      "metadata": {
        "id": "tL4kcT0xgR0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gOJsSGoGfYA4"
      },
      "outputs": [],
      "source": [
        "# Dependencies to support interactive charting in a Colab Notebook\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Import core AutoGen and AutoGen Coding library\n",
        "import autogen\n",
        "from autogen.coding import LocalCommandLineCodeExecutor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRvt2QhGfYA5"
      },
      "source": [
        "## Example Task: Check Stock Price Change\n",
        "\n",
        "In the example below, let's see how to use the agents in AutoGen to write a python script and execute the script. This process involves constructing a `AssistantAgent` to serve as the assistant, along with a `UserProxyAgent` that acts as a proxy for the human user. In this example demonstrated below, when constructing the `UserProxyAgent`,  we select the `human_input_mode` to \"NEVER\". This means that the `UserProxyAgent` will not solicit feedback from the human user. It stops replying when the limit defined by `max_consecutive_auto_reply` is reached, or when `is_termination_msg()` returns true for the received message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1NcVLV2fYA6"
      },
      "outputs": [],
      "source": [
        "# create an AssistantAgent named \"assistant\"\n",
        "assistant = autogen.AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    llm_config={\n",
        "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
        "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
        "        \"temperature\": 0,  # temperature for sampling\n",
        "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
        ")\n",
        "\n",
        "# create a UserProxyAgent instance named \"user_proxy\"\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    code_execution_config={\n",
        "        # the executor to run the generated code\n",
        "        \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
        "    },\n",
        ")\n",
        "# the assistant receives a message from the user_proxy, which contains the task description\n",
        "chat_res = user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"\"\"What date is today? Compare the year-to-date gain for BAE SYSTEMS and Lockheed Martin.\"\"\",\n",
        "    summary_method=\"reflection_with_llm\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSWB7kSJfYA6"
      },
      "source": [
        "The example above involves code execution. In AutoGen, code execution is triggered automatically by the `UserProxyAgent` when it detects an executable code block in a received message and no human user input is provided.\n",
        "Users have the option to specify a different working directory by setting the `work_dir` argument when constructing a new instance of the `LocalCommandLineCodeExecutor`.\n",
        "For Docker-based or Jupyter kernel-based code execution, please refer to\n",
        "[Code Executors Tutorial](/docs/tutorial/code-executors) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9YDvKwZfYA7"
      },
      "source": [
        "#### Check chat results\n",
        "The `initiate_chat` method returns a `ChatResult` object, which is a dataclass object storing information about the chat. Currently, it includes the following attributes:\n",
        "\n",
        "- `chat_history`: a list of chat history.\n",
        "- `summary`: a string of chat summary. A summary is only available if a summary_method is provided when initiating the chat.\n",
        "- `cost`: a tuple of (total_cost, total_actual_cost), where total_cost is a dictionary of cost information, and total_actual_cost is a dictionary of information on the actual incurred cost with cache.\n",
        "- `human_input`: a list of strings of human inputs solicited during the chat. (Note that since we are setting `human_input_mode` to `NEVER` in this notebook, this list is always empty.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVrllr94fYA7"
      },
      "outputs": [],
      "source": [
        "print(\"Chat history:\", chat_res.chat_history)\n",
        "\n",
        "print(\"Summary:\", chat_res.summary)\n",
        "print(\"Cost info:\", chat_res.cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3YHa0iAfYA7"
      },
      "source": [
        "## Example Task: Plot Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kyLuDWOfYA7"
      },
      "outputs": [],
      "source": [
        "# followup of the previous question\n",
        "user_proxy.send(\n",
        "    recipient=assistant,\n",
        "    message=\"\"\"Plot a chart of their stock price change YTD. Save the data to stock_price_ytd.csv, and save the plot to stock_price_ytd.png.\"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ygNVL2pfYA8"
      },
      "source": [
        "Let's display the generated figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSKu7iJkfYA8"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    image = Image(filename=\"coding/stock_price_ytd.png\")\n",
        "    display(image)\n",
        "except FileNotFoundError:\n",
        "    print(\"Image not found. Please check the file name and modify if necessary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjo_Ac1EfYA8"
      },
      "source": [
        "Let's display the raw data collected and saved from previous chat as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCusZhXnfYA8"
      },
      "outputs": [],
      "source": [
        "# Path to your CSV file\n",
        "file_path = \"coding/stock_price_ytd.csv\"\n",
        "try:\n",
        "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
        "        # Read each line in the file\n",
        "        for line in file:\n",
        "            # Split the line into a list using the comma as a separator\n",
        "            row = line.strip().split(\",\")\n",
        "            # Print the list representing the current row\n",
        "            print(row)\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the file name and modify if necessary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vblGCqFQfYA8"
      },
      "source": [
        "## Example Task: Use User Defined Message Function to let Agents Analyze data Collected\n",
        "\n",
        "Let's create a user defined message to let the agents analyze the raw data and write a blogpost. The function is supposed to take `sender`, `recipient` and `context` as inputs and outputs a string of message.\n",
        "\n",
        "**kwargs from `initiate_chat` will be used as `context`. Take the following code as an example, the `context` includes a field `file_name` as provided in `initiate_chat`. In the user defined message function `my_message_generator`, we are reading data from the file specified by this filename.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXetaItJfYA8"
      },
      "outputs": [],
      "source": [
        "def my_message_generator(sender, recipient, context):\n",
        "    # your CSV file\n",
        "    file_name = context.get(\"file_name\")\n",
        "    try:\n",
        "        with open(file_name, mode=\"r\", encoding=\"utf-8\") as file:\n",
        "            file_content = file.read()\n",
        "    except FileNotFoundError:\n",
        "        file_content = \"No data found.\"\n",
        "    return \"Analyze the data and write a brief but engaging blog post. \\n Data: \\n\" + file_content\n",
        "\n",
        "\n",
        "# followup of the previous question\n",
        "chat_res = user_proxy.initiate_chat(\n",
        "    recipient=assistant,\n",
        "    message=my_message_generator,\n",
        "    file_name=\"coding/stock_price_ytd.csv\",\n",
        "    summary_method=\"reflection_with_llm\",\n",
        "    summary_args={\"summary_prompt\": \"Return the blog post in Markdown format.\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB3uzUd8fYA8"
      },
      "source": [
        "Let's check the summary of the chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y0Zn7t8fYA8"
      },
      "outputs": [],
      "source": [
        "print(chat_res.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_9FbCbyfYA9"
      },
      "source": [
        "Let's check how much the above chat cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xHYyxaHfYA9"
      },
      "outputs": [],
      "source": [
        "print(chat_res.cost)"
      ]
    }
  ],
  "metadata": {
    "front_matter": {
      "description": "Use conversable language learning model agents to solve tasks and provide automatic feedback through a comprehensive example of writing, executing, and debugging Python code to compare stock price changes.",
      "tags": [
        "code generation",
        "debugging"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}